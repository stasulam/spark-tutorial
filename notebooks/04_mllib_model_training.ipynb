{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "#### MLLib docs:\n",
    "https://spark.apache.org/docs/2.3.2/ml-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if we have SparkSession active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hive-cluster-m.c.getindata-training.internal:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f43145e17f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most important facts about Spark MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* works only with numeric data\n",
    "* models accepts training data and labels as two columns. For labels it is normal, but training data has to be vector of variables you want to use. Below you will learn how to build such vectors\n",
    "* API is very similar to scikit-learn with .fit and .transform methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For sample execution we will load original data. After checking how it works I would like you to load data prepared in previous step and build model based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = spark.sql(\"\"\"SELECT taxi_id,\n",
    "                        trip_start_timestamp,\n",
    "                        trip_end_timestamp,\n",
    "                        trip_seconds,\n",
    "                        trip_miles,\n",
    "                        pickup_census_tract,\n",
    "                        dropoff_census_tract,\n",
    "                        pickup_community_area,\n",
    "                        dropoff_community_area,\n",
    "                        fare,\n",
    "                        tips,\n",
    "                        tolls,\n",
    "                        extras,\n",
    "                        trip_total,\n",
    "                        company,\n",
    "                        IF(payment_type='Credit Card',1,0) target\n",
    "                    FROM\n",
    "                        tomek.taxi_cleaned\n",
    "                    WHERE\n",
    "                        yyyymm BETWEEN 201601 AND 201612\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+------------+---------------------+----------------------+\n",
      "|target|             company|  fare|trip_seconds|pickup_community_area|dropoff_community_area|\n",
      "+------+--------------------+------+------------+---------------------+----------------------+\n",
      "|     0|                    |2925.0|        1020|                   32|                    56|\n",
      "|     1|Taxi Affiliation ...|1850.0|        1860|                   33|                     8|\n",
      "|     1|                    |1250.0|         960|                   33|                    28|\n",
      "|     0|Dispatch Taxi Aff...| 825.0|         720|                    8|                     8|\n",
      "+------+--------------------+------+------------+---------------------+----------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we will show only first 4 columns for readability\n",
    "taxi = taxi.select('target','company','fare','trip_seconds','pickup_community_area','dropoff_community_area')\n",
    "taxi.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic statistics in MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have all numeric features stored as numeric as spark is accepting only numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = false)\n",
      " |-- company: string (nullable = true)\n",
      " |-- fare: float (nullable = true)\n",
      " |-- trip_seconds: long (nullable = true)\n",
      " |-- pickup_community_area: long (nullable = true)\n",
      " |-- dropoff_community_area: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is nothing to convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rid of null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in initial show statement there are some null values in our data. We have to get rid of them, because null values are not accepted in some spark methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.filter(col(\"fare\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1447175"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.filter(col(\"pickup_community_area\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1716431"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.filter(col(\"dropoff_community_area\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15061895"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = false)\n",
      " |-- company: string (nullable = true)\n",
      " |-- fare: float (nullable = true)\n",
      " |-- trip_seconds: long (nullable = true)\n",
      " |-- pickup_community_area: long (nullable = true)\n",
      " |-- dropoff_community_area: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.na.fill(-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.filter(col(\"pickup_community_area\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.filter(col(\"dropoff_community_area\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector assembler is used for building column of vectors of variables. Most MLLib algos accepts only such input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['fare','trip_seconds','pickup_community_area','dropoff_community_area'],\n",
    "    outputCol=\"numeric_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+---------------------+----------------------+--------------------+\n",
      "|  fare|trip_seconds|pickup_community_area|dropoff_community_area|    numeric_features|\n",
      "+------+------------+---------------------+----------------------+--------------------+\n",
      "| 800.0|         420|                   24|                     8|[800.0,420.0,24.0...|\n",
      "|1750.0|        1080|                    8|                     6|[1750.0,1080.0,8....|\n",
      "| 800.0|         660|                    8|                    32|[800.0,660.0,8.0,...|\n",
      "| 725.0|         420|                   24|                     8|[725.0,420.0,24.0...|\n",
      "| 675.0|         360|                    8|                    24|[675.0,360.0,8.0,...|\n",
      "| 525.0|         240|                   32|                    32|[525.0,240.0,32.0...|\n",
      "|1350.0|         921|                   24|                     8|[1350.0,921.0,24....|\n",
      "| 875.0|         660|                    8|                    24|[875.0,660.0,8.0,...|\n",
      "|4775.0|        4440|                   76|                     8|[4775.0,4440.0,76...|\n",
      "| 850.0|         540|                    8|                    24|[850.0,540.0,8.0,...|\n",
      "+------+------------+---------------------+----------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler\\\n",
    "    .transform(taxi)\\\n",
    "    .select('fare',\n",
    "            'trip_seconds',\n",
    "            'pickup_community_area',\n",
    "            'dropoff_community_area',\n",
    "            'numeric_features')\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please remember that to save such operation you have to overwrite original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = assembler.transform(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarization is the process of thresholding numerical features to binary (0/1) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add column indicating if trip time is higher or lower than average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|avg(trip_seconds)|\n",
      "+-----------------+\n",
      "|863.3475186887174|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's calculate average trip time\n",
    "taxi.select(avg(col(\"trip_seconds\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "863.3475186887174"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now extract time\n",
    "taxi.select(avg(col(\"trip_seconds\"))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "863.3475186887174"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.select(avg(col(\"trip_seconds\"))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.withColumn(\"trip_seconds_double\",col(\"trip_seconds\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=taxi.select(avg(col(\"trip_seconds\"))).collect()[0][0],\n",
    "                      inputCol=\"trip_seconds_double\",\n",
    "                      outputCol=\"binarized_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|trip_seconds|binarized_features|\n",
      "+------------+------------------+\n",
      "|         420|               0.0|\n",
      "|         180|               0.0|\n",
      "|         720|               0.0|\n",
      "|         660|               0.0|\n",
      "|         360|               0.0|\n",
      "|         660|               0.0|\n",
      "|         360|               0.0|\n",
      "|         300|               0.0|\n",
      "|        2460|               1.0|\n",
      "|         180|               0.0|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binarizer.transform(taxi).select(\"trip_seconds\",\"binarized_features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again save results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = binarizer.transform(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also binarize whole vector at once. For this case we will use features vector created in last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=0.5, inputCol=\"numeric_features\", outputCol=\"binarized_features_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    numeric_features|binarized_features_2|\n",
      "+--------------------+--------------------+\n",
      "|[3325.0,1740.0,-9...|   [1.0,1.0,0.0,0.0]|\n",
      "|[4475.0,2340.0,-9...|   [1.0,1.0,0.0,0.0]|\n",
      "|[1800.0,960.0,-99...|   [1.0,1.0,0.0,0.0]|\n",
      "|[650.0,540.0,-99....|   [1.0,1.0,0.0,0.0]|\n",
      "|[1575.0,780.0,-99...|   [1.0,1.0,0.0,0.0]|\n",
      "|[4475.0,1560.0,-9...|   [1.0,1.0,0.0,0.0]|\n",
      "|[765.0,540.0,8.0,...|   [1.0,1.0,1.0,1.0]|\n",
      "|[425.0,180.0,8.0,...|   [1.0,1.0,1.0,1.0]|\n",
      "|[1005.0,600.0,-99...|   [1.0,1.0,0.0,0.0]|\n",
      "|[1125.0,1260.0,-9...|   [1.0,1.0,0.0,0.0]|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binarizer.transform(taxi).select(\"numeric_features\",\"binarized_features_2\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will not save it as it does not make any sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLLib is accepting only numeric variables as input. We have to convert any categorical columns to numeric ones. We will use string indexer for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0. The unseen labels will be put at index numLabels if user chooses to keep them. If the input column is numeric, we cast it to string and index the string values. When downstream pipeline components such as Estimator or Transformer make use of this string-indexed label, you must set the input column of the component to this string-indexed column name. In many cases, you can set the input column with setInputCol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again check for strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = false)\n",
      " |-- company: string (nullable = true)\n",
      " |-- fare: float (nullable = false)\n",
      " |-- trip_seconds: long (nullable = true)\n",
      " |-- pickup_community_area: long (nullable = true)\n",
      " |-- dropoff_community_area: long (nullable = true)\n",
      " |-- numeric_features: vector (nullable = true)\n",
      " |-- trip_seconds_double: double (nullable = true)\n",
      " |-- binarized_features: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             company|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|Taxi Affiliation ...|\n",
      "|Taxi Affiliation ...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.select('company').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have empty strings. If yes we have to put there some kind of representation as it is needed for further usage. Some string methods in spark does not accept variables with empty strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|             company|  count|\n",
      "+--------------------+-------+\n",
      "|                    |7306268|\n",
      "|Taxi Affiliation ...|3538219|\n",
      "|Dispatch Taxi Aff...|1465481|\n",
      "|Choice Taxi Assoc...| 975823|\n",
      "|Northwest Managem...| 439672|\n",
      "|Blue Ribbon Taxi ...| 337064|\n",
      "|KOAM Taxi Associa...| 317938|\n",
      "| Top Cab Affiliation| 286144|\n",
      "|Chicago Medallion...| 201350|\n",
      "|Chicago Medallion...|  72284|\n",
      "|6743 - 78771 Luha...|   6214|\n",
      "|        5129 - 87128|   5619|\n",
      "|0118 - 42111 Godf...|   4982|\n",
      "|3141 - 87803 Zip Cab|   4912|\n",
      "|1085 - 72312 N an...|   4701|\n",
      "|3011 - 66308 JBL ...|   4475|\n",
      "|2092 - 61288 Sbei...|   4428|\n",
      "|6574 - Babylon Ex...|   4253|\n",
      "|5724 - 75306 KYVI...|   4065|\n",
      "|3152 - 97284 Crys...|   3750|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.groupBy(\"company\").count().sort(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is most popular value. Let's populate it with something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.withColumn(\"company\",when(col(\"company\")==\"\",\"empty\").otherwise(col(\"company\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|             company|  count|\n",
      "+--------------------+-------+\n",
      "|               empty|7306268|\n",
      "|Taxi Affiliation ...|3538219|\n",
      "|Dispatch Taxi Aff...|1465481|\n",
      "|Choice Taxi Assoc...| 975823|\n",
      "|Northwest Managem...| 439672|\n",
      "|Blue Ribbon Taxi ...| 337064|\n",
      "|KOAM Taxi Associa...| 317938|\n",
      "| Top Cab Affiliation| 286144|\n",
      "|Chicago Medallion...| 201350|\n",
      "|Chicago Medallion...|  72284|\n",
      "|6743 - 78771 Luha...|   6214|\n",
      "|        5129 - 87128|   5619|\n",
      "|0118 - 42111 Godf...|   4982|\n",
      "|3141 - 87803 Zip Cab|   4912|\n",
      "|1085 - 72312 N an...|   4701|\n",
      "|3011 - 66308 JBL ...|   4475|\n",
      "|2092 - 61288 Sbei...|   4428|\n",
      "|6574 - Babylon Ex...|   4253|\n",
      "|5724 - 75306 KYVI...|   4065|\n",
      "|3152 - 97284 Crys...|   3750|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.groupBy(\"company\").count().sort(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer(inputCol=\"company\",outputCol=\"company_indexed\")\n",
    "taxi = string_indexer.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label 0 is assigned to most popular value, label 1 for next and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, there are three strategies regarding how StringIndexer will handle unseen labels when you have fit a StringIndexer on one dataset and then use it to transform another:\n",
    "* throw an exception (which is the default)\n",
    "* skip the row containing the unseen label entirely\n",
    "* put unseen labels in a special additional bucket, at index numLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algos are expecting features to be continues ones - like Logistic regression. We have to encode our categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. For string type input data, it is common to encode categorical features using StringIndexer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please modify to transform column you have used. It can transform multiple columns. Just put them in list.\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"company_indexed\"],\n",
    "                                 outputCols=[\"company_indexed_vec\"],\n",
    "                                 handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|company_indexed|company_indexed_vec|\n",
      "+---------------+-------------------+\n",
      "|            0.0|     (56,[0],[1.0])|\n",
      "|            0.0|     (56,[0],[1.0])|\n",
      "|            0.0|     (56,[0],[1.0])|\n",
      "|            3.0|     (56,[3],[1.0])|\n",
      "|            3.0|     (56,[3],[1.0])|\n",
      "|            4.0|     (56,[4],[1.0])|\n",
      "|            0.0|     (56,[0],[1.0])|\n",
      "|            0.0|     (56,[0],[1.0])|\n",
      "|            2.0|     (56,[2],[1.0])|\n",
      "|            0.0|     (56,[0],[1.0])|\n",
      "+---------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.fit(taxi).transform(taxi).select(\"company_indexed\",\"company_indexed_vec\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = encoder.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will gather all features as MLLib is accepting only feature vector as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = false)\n",
      " |-- company: string (nullable = true)\n",
      " |-- fare: float (nullable = false)\n",
      " |-- trip_seconds: long (nullable = true)\n",
      " |-- pickup_community_area: long (nullable = true)\n",
      " |-- dropoff_community_area: long (nullable = true)\n",
      " |-- numeric_features: vector (nullable = true)\n",
      " |-- trip_seconds_double: double (nullable = true)\n",
      " |-- binarized_features: double (nullable = true)\n",
      " |-- company_indexed: double (nullable = false)\n",
      " |-- company_indexed_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_assembler = VectorAssembler(inputCols=[\"numeric_features\",\n",
    "                                               \"binarized_features\",\n",
    "                                               \"company_indexed_vec\"],\n",
    "                                    outputCol=\"training_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   training_features|\n",
      "+--------------------+\n",
      "|(61,[0,1,2,3,5],[...|\n",
      "|(61,[0,1,2,3,4,5]...|\n",
      "|(61,[0,1,2,3,5],[...|\n",
      "|(61,[0,1,2,3,8],[...|\n",
      "|(61,[0,1,2,3,8],[...|\n",
      "|(61,[0,1,2,3,9],[...|\n",
      "|(61,[0,1,2,3,4,5]...|\n",
      "|(61,[0,1,2,3,5],[...|\n",
      "|(61,[0,1,2,3,4,7]...|\n",
      "|(61,[0,1,2,3,5],[...|\n",
      "|(61,[0,1,2,3,7],[...|\n",
      "|(61,[0,1,2,3,4,5]...|\n",
      "|(61,[0,1,2,3,5],[...|\n",
      "|(61,[0,1,2,3,4,5]...|\n",
      "|(61,[0,1,2,3,9],[...|\n",
      "|(61,[0,1,2,3,4,6]...|\n",
      "|(61,[0,1,2,3,5],[...|\n",
      "|(61,[0,1,2,3,5],[...|\n",
      "|(61,[0,1,2,3,9],[...|\n",
      "|(61,[0,1,2,3,4,6]...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_assembler.transform(taxi).select(\"training_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = feature_assembler.transform(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's prepare label now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi \\\n",
    "    .withColumn(\"target\",taxi.target.cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And train first model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = taxi.randomSplit([0.7, 0.3],seed=1254129345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"target\", featuresCol=\"training_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0002601895981080373,-2.6717119402180077e-05,0.0020725557511046323,-0.000684387006512551,0.07664988469450165,-0.3004405781570859,-0.2668672699268945,-0.06769330257006727,0.052594491093831915,-0.07810495796184856,-0.17201551578280883,-0.22616775645757659,-0.36103002528321143,-0.3457422934426323,-0.35833254539856474,-1.4136468195545173,0.1596929872001298,-0.7687137542437171,-0.3323240351863917,-0.31625049924330106,0.09046467491945548,-5.071261369160556,-0.019706944889140373,-0.06427560196830057,-0.5100102592724889,-0.20474427863371567,-0.29361702695327685,-0.1619791640384127,0.12301203595031768,-1.1028813448242347,-0.17752059354614438,-0.12774403917696137,-0.40254338553778385,-0.587488800705374,-0.20753290524457177,-0.7446846339243893,0.010012199177659581,-0.37937009242837655,-0.03912486010517421,-0.3506676257877196,0.040918413295948274,-1.110641714688644,-0.5724476269424014,-0.47597134685117953,-0.16273944826372905,-1.5290306434422898,0.0735526982548517,0.11366331609206055,-0.6681334352309852,0.23189185064923912,-0.6600907919484004,-0.23768906575125956,0.14217842229093797,0.6210180855478085,-1.0625119632348274,0.160696350650006,-1.3096455116351582,-1.0549741585712882,0.6565948530947305,-7.664442082869544,6.374638068149501]\n",
      "Intercept: -0.3047337245821163\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+\n",
      "|prediction|target|   training_features|\n",
      "+----------+------+--------------------+\n",
      "|       0.0|   0.0|(61,[0,1,2,3,17],...|\n",
      "|       0.0|   0.0|(61,[0,1,2,3,17],...|\n",
      "|       0.0|   0.0|(61,[0,1,2,3,17],...|\n",
      "|       0.0|   0.0|(61,[0,1,2,3,17],...|\n",
      "|       0.0|   0.0|(61,[0,1,2,3,17],...|\n",
      "+----------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"target\", \"training_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How good we are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.408599 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check some trivial solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|target|  count|\n",
      "+------+-------+\n",
      "|   0.0|8099718|\n",
      "|   1.0|6962177|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.groupBy(\"target\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumn(\"all_zeros\",lit(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.462161 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"all_zeros\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to improve our results with CrossValidation and Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0]) \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.40878 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I would like you to work with data prepared in previous notebook and train your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}